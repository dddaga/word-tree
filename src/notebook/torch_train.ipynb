{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_DIMENSIN   = 75\n",
    "CONTEXT_DECAY      = 0.5\n",
    "CONTRASTIVE_WEIGHT = 0.1\n",
    "LEANING_RATE       = 1\n",
    "DROPOUT            = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient('mongodb://localhost:27017')\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "collection = mydb.p_and_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ngram(indices, window_size=2):\n",
    "    ngrams = []\n",
    "    count = 0\n",
    "    for token in indices[:len(indices)-window_size+1]:\n",
    "        ngrams.append(indices[count:count+window_size])\n",
    "        count = count+1\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "\n",
    "def touch_connection_db(collection, word_1, word_2):\n",
    "    find_connection = collection.find_one({'word': word_1, 'connection': word_2})\n",
    "    if find_connection is None:\n",
    "        # print('connection {} {} is new'.format(word_1,word_2))\n",
    "        context_vector = np.random.rand(CONTEXT_DIMENSIN)\n",
    "        unit_context_vector = context_vector / np.linalg.norm(context_vector)\n",
    "        connection = {'word': word_1,\n",
    "                      'connection': word_2,\n",
    "                      'frequency': 1,\n",
    "                      'context': list(unit_context_vector),\n",
    "                      'update_count': 0,\n",
    "                      'lock': False}\n",
    "\n",
    "        collection.insert_one(connection)\n",
    "        return connection\n",
    "    else:\n",
    "        # print('connection {} {} is old'.format(word_1,word_2))\n",
    "        find_connection['frequency'] +=  1\n",
    "        frequecny = find_connection['frequency']\n",
    "        \n",
    "        collection.update_one({'word': word_1, 'connection': word_2},\n",
    "                              {'$set': {'frequency': frequecny}})\n",
    "        \n",
    "        return find_connection\n",
    "        \n",
    "\n",
    "def update_graph_context(x,update_count=False):\n",
    "    \n",
    "    if update_count :\n",
    "        collection.update_one({'word': x['word'], 'connection': x['connection']},\n",
    "                              {'$set': {'context':list(x['updated_context']),'update_count':x['update_count']+1 }})\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        collection.update_one({'word': x['word'], 'connection': x['connection']},\n",
    "                              {'$set': {'context':list(x['updated_context']) }})\n",
    "        \n",
    "# def train_policy(target,connection_tensors,context_vector)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5022e9c5fff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#def  train_graph(context_vector,train_step,policy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mneighbours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcontext_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_step' is not defined"
     ]
    }
   ],
   "source": [
    "context_vector = torch.zeros(CONTEXT_DIMENSIN)     \n",
    "# def train_policy(target,connection_tensors,context_vector)\n",
    "        \n",
    "    #input train step , context_vector\n",
    "#output  return context_vector; updates graph \n",
    "\n",
    "#def  train_graph(context_vector,train_step,policy)\n",
    "\n",
    "neighbours = get_ngram(train_step,2)\n",
    "context_history = neighbours[:-1]\n",
    "target = neighbours[-1]\n",
    "\n",
    "\n",
    "connection_tensors = []\n",
    "drop = torch.nn.Dropout(p=DROPOUT)\n",
    "connecions = []\n",
    "\n",
    "for neighbour in context_history:\n",
    "    connection = touch_connection_db(collection,neighbour[0],neighbour[1])\n",
    "    a = torch.tensor(connection['context'])\n",
    "    a.requires_grad_(True)\n",
    "    connection_tensors.append(a)\n",
    "    connecions.append(connection)\n",
    "    \n",
    "    \n",
    "    context_step = a.add(drop(context_vector))  \n",
    "    context_vector = context_step / context_step.norm()\n",
    "\n",
    "    \n",
    "target_connection = touch_connection_db(collection,target[0],target[1])\n",
    "target_context    = torch.tensor(target_connection['context'])\n",
    "target_context.requires_grad_(True)\n",
    "\n",
    "\n",
    "\n",
    "primary_error  =  context_vector - target_context\n",
    "primary_loss   =  primary_error.square().sum()\n",
    "\n",
    "\n",
    "\n",
    "#secondary error\n",
    "connection_df = pd.DataFrame(collection.find({'word':target[0]}))\n",
    "connection_count = len(connection_df)\n",
    "negative_df = connection_df.loc[connection_df['connection'] != target[1]]\n",
    "\n",
    "\n",
    "\n",
    "negative_tensors = []\n",
    "contrastive_loss = torch.zeros(1)\n",
    "\n",
    "\n",
    "for negative in negative_df.iterrows():\n",
    "    connection = touch_connection_db(collection,neighbour[0],neighbour[1])\n",
    "    c = torch.tensor(connection['context'])\n",
    "    c.requires_grad_(True)\n",
    "    negative_tensors.append(c)\n",
    "    \n",
    "    contrast_error = context_vector - drop(c)\n",
    "    contrastive_loss += contrast_error.square().sum()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = torch.zeros(CONTEXT_DIMENSIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input train step , context_vector\n",
    "#output  return context_vector; updates graph \n",
    "\n",
    "#def  train_graph(context_vector,train_step,policy)\n",
    "\n",
    "train_step = ['the', 'same','bfbrbijfjbfdkjg', 'time', '.', 'I', 'am']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af7d5be13805>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'context_history' is not defined"
     ]
    }
   ],
   "source": [
    "context_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = get_ngram(train_step,2)\n",
    "\n",
    "context_history = neighbours[:-1]\n",
    "target = neighbours[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'same'],\n",
       " ['same', 'bfbrbijfjbfdkjg'],\n",
       " ['bfbrbijfjbfdkjg', 'time'],\n",
       " ['time', '.'],\n",
       " ['.', 'I'],\n",
       " ['I', 'am']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_tensors = []\n",
    "drop = torch.nn.Dropout(p=DROPOUT)\n",
    "context_trajectory = []\n",
    "\n",
    "for neighbour in context_history:\n",
    "    connection = touch_connection_db(collection,neighbour[0],neighbour[1])\n",
    "    a = torch.tensor(connection['context'])\n",
    "    a.requires_grad_(True)\n",
    "    connection_tensors.append(a)\n",
    "    context_trajectory.append(connection)\n",
    "    \n",
    "    \n",
    "    context_step = a.add(drop(context_vector))  \n",
    "    context_vector = context_step / context_step.norm()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5fbbda6dfacbd9ec2a50c866'),\n",
       " 'word': 'I',\n",
       " 'connection': 'am',\n",
       " 'frequency': 311,\n",
       " 'context': [-0.08540778345851191,\n",
       "  0.08143379745666982,\n",
       "  -0.24326798640745542,\n",
       "  -0.1928139566881557,\n",
       "  0.01511869907179042,\n",
       "  0.15831460796895783,\n",
       "  0.030732382640587835,\n",
       "  0.3179370500398271,\n",
       "  0.6933729510034384,\n",
       "  0.36421136916780394,\n",
       "  -0.1601258975254265,\n",
       "  0.21890916493541135,\n",
       "  0.4402192038626021,\n",
       "  -0.123273263646342,\n",
       "  -0.353985255220992,\n",
       "  -0.09375432131743934,\n",
       "  -0.4203985727275802,\n",
       "  0.5294319867394139,\n",
       "  -0.03590380893588352,\n",
       "  0.298969661540848,\n",
       "  0.29577661394707094,\n",
       "  0.06951516213404765,\n",
       "  0.5582259832045231,\n",
       "  0.19517314645579306,\n",
       "  -0.12146672949310916,\n",
       "  0.09436008453488062,\n",
       "  0.47171383080370216,\n",
       "  0.2615919204939391,\n",
       "  0.16481992435200823,\n",
       "  -0.21403115072598883,\n",
       "  -0.02069483072598116,\n",
       "  -0.02219495187459239,\n",
       "  -0.10875000201795121,\n",
       "  -0.09111942349598107,\n",
       "  -0.2435879188886674,\n",
       "  -0.09363591905774171,\n",
       "  0.4412914666638569,\n",
       "  0.6662093687851828,\n",
       "  0.46648743275324933,\n",
       "  -0.036242492623301176,\n",
       "  -0.158252555760395,\n",
       "  0.20830312275202978,\n",
       "  0.03275727168906056,\n",
       "  0.05604089109225646,\n",
       "  0.6814762852637504,\n",
       "  0.22209600814999125,\n",
       "  0.3091135568473183,\n",
       "  0.3562429069525226,\n",
       "  -0.23580063372168614,\n",
       "  0.4639032267800831,\n",
       "  -0.07531585390589665,\n",
       "  0.35478663713188263,\n",
       "  0.5370361706540884,\n",
       "  -0.14933995812346249,\n",
       "  0.04281679352928061,\n",
       "  -0.43565281659307326,\n",
       "  0.6647425756804866,\n",
       "  0.12780569747222542,\n",
       "  0.4903535038375727,\n",
       "  0.4647344284973756,\n",
       "  -0.04123153214930583,\n",
       "  0.5650774102217742,\n",
       "  0.40835129191587405,\n",
       "  -0.503402238778528,\n",
       "  -0.12474498363360761,\n",
       "  0.10926845121663015,\n",
       "  0.30035089256436287,\n",
       "  0.13833212585550084,\n",
       "  0.20474747751713895,\n",
       "  0.3176543162728991,\n",
       "  -0.08775371815066393,\n",
       "  0.12855778883051516,\n",
       "  -0.09612809632063826,\n",
       "  0.5341554889836315,\n",
       "  -0.16033538157854083],\n",
       " 'update_count': 3,\n",
       " 'lock': False}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# def train_policy(target,connection_tensors,context_vector)\n",
    "target_connection = touch_connection_db(collection,target[0],target[1])\n",
    "target_context    = torch.tensor(target_connection['context'])\n",
    "target_context.requires_grad_(True)\n",
    "\n",
    "\n",
    "\n",
    "primary_error  =  context_vector - target_context\n",
    "primary_loss   =  primary_error.square().sum()\n",
    "\n",
    "\n",
    "\n",
    "#decay updates for primary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#secondary error\n",
    "connection_df = pd.DataFrame(collection.find({'word':target[0]}))\n",
    "connection_count = len(connection_df)\n",
    "negative_df = connection_df.loc[connection_df['connection'] != target[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-dbf63601c28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mprimary_loss\u001b[0m \u001b[0;34m-\u001b[0m  \u001b[0mCONTRASTIVE_WEIGHT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcontrastive_loss\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mconnection_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/py_envs/Anuvaad/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/py_envs/Anuvaad/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "negative_tensors = []\n",
    "contrastive_loss = 0\n",
    "\n",
    "\n",
    "for index,negative in negative_df.iterrows():\n",
    "   \n",
    "    c = torch.tensor(negative['context'])\n",
    "    c.requires_grad_(True)\n",
    "    negative_tensors.append(c)\n",
    "    \n",
    "    contrast_error = context_vector - drop(c)\n",
    "    contrastive_loss += contrast_error.square().sum()\n",
    "    \n",
    "loss =   primary_loss -  CONTRASTIVE_WEIGHT * (contrastive_loss/ connection_count)\n",
    "\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary loss is tensor(0.3795, grad_fn=<SumBackward0>)\n",
      "contrastive loss is  tensor(0.2175, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('primary loss is', primary_loss )\n",
    "print('contrastive loss is ',contrastive_loss / connection_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update context:\n",
    "connection_count = len(context_trajectory)\n",
    "for index, connection in enumerate(context_trajectory):\n",
    "        \n",
    "    update_count = connection['update_count']\n",
    "\n",
    "    gradient = connection_tensors[index].grad.numpy()\n",
    "    weight = LEANING_RATE  * (CONTEXT_DECAY **(connection_count - index)) * (1/np.sqrt(update_count +1))\n",
    "    connection['updated_context'] = connection['context'] - weight * gradient\n",
    "    \n",
    "    update_graph_context(connection)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gradient = target_context.grad.numpy()\n",
    "traget_update_count = target_connection['update_count'] +1\n",
    "target_connection['updated_context'] = target_connection['context'] - \\\n",
    "                         LEANING_RATE * (1/np.sqrt(traget_update_count))*target_gradient\n",
    "\n",
    "\n",
    "update_graph_context(target_connection,update_count=True)\n",
    "\n",
    "\n",
    "# collection.update_one({'word': target_connection['word'], 'connection': target_connection['connection']},\n",
    "#                               {'$set': {'update_count': traget_update_count,'context':list(updated_target_context) }})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhiraj/py_envs/Anuvaad/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/dhiraj/py_envs/Anuvaad/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "negative_df['tensors'] = negative_tensors\n",
    "negative_df['updated_context'] = negative_df['context'] - \\\n",
    "                                  LEANING_RATE * negative_df['tensors'].apply(lambda x :x.grad.numpy())\n",
    "negative_df.apply(update_graph_context,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anuvaad",
   "language": "python",
   "name": "anuvaad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
